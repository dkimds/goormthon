{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkimds/goormthon/blob/main/naver_news_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO-ZEo4TUWLH"
      },
      "source": [
        "# 네이버 뉴스 크롤러 만들기 연습\n",
        "아래 코드를 실행해야 이하부분이 돌아갑니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUc2XeYvLZvW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXBYxZIF_pB5"
      },
      "source": [
        "# 제출 평가 부분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi0NWviPJblt"
      },
      "outputs": [],
      "source": [
        "def main_crawling(query, start_date, end_date, sort_type, max_page):\n",
        "    '''\n",
        "    뉴스를 검색하여 데이터 프레임으로 반환\n",
        "    input args::\n",
        "        query: 검색하려는 뉴스주제\n",
        "        start_date: 뉴스검색이 시작되는 날짜\n",
        "        end_date: 뉴스검색이 끝나는 날짜\n",
        "        sort_type: 정렬타입. 관련도순 = 0, 최신순 = 1, 오래된순 = 2\n",
        "        max_page: 스크래핑 원하는 페이지 갯수\n",
        "    return:\n",
        "        데이터 필드: 날짜, 카테고리, 발행사, 제목, 기사문, 기사주소\n",
        "    '''\n",
        "    if query == '':\n",
        "        query = '데이터 분석'\n",
        "    if len(start_date) != 10:\n",
        "        start_date = '2021.01.01'\n",
        "    if len(end_date) != 10:\n",
        "        end_date = '2021.12.31'\n",
        "    if sort_type not in ['0', '1', '2']:\n",
        "        sort_type = '0'\n",
        "    if max_page == '':\n",
        "        max_page = 5\n",
        "\n",
        "\n",
        "    # 각 기사들의 데이터를 종류별로 나눠담을 리스트를 생성합니다. (추후 DataFrame으로 모을 예정)\n",
        "    categories = []\n",
        "    titles = []\n",
        "    dates = []\n",
        "    articles = []\n",
        "    article_urls = []\n",
        "    press_companies = []\n",
        "\n",
        "    # 주어진 일자를 쿼리에 맞는 형태로 변경해줍니다.\n",
        "    start_date = start_date.replace(\".\", \"\")\n",
        "    end_date = end_date.replace(\".\", \"\")\n",
        "\n",
        "    # 지정한 기간 내 원하는 페이지 수만큼의 기사를 크롤링합니다.\n",
        "    current_call = 1\n",
        "    last_call = (max_page - 1) * 10 + 1 # max_page이 5일 경우 41에 해당\n",
        "\n",
        "\n",
        "    while current_call <= last_call:\n",
        "\n",
        "        print('\\n{}번째 기사글부터 크롤링을 시작합니다.'.format(current_call))\n",
        "\n",
        "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query \\\n",
        "              + \"&sort=\" + sort_type \\\n",
        "              + \"&nso=so%3Ar%2Cp%3Afrom\" + start_date \\\n",
        "              + \"to\" + end_date \\\n",
        "              + \"%2Ca%3A&start=\" + str(current_call)\n",
        "\n",
        "        urls_list = []\n",
        "        try: # 네이버 뉴스 검색결과 페이지 자체에 접근이 불가능할 경우 에러가 발생할 수 있습니다.\n",
        "            web = requests.get(url).content\n",
        "            source = BeautifulSoup(web, 'html.parser')\n",
        "\n",
        "            for urls in source.find_all('a', {'class' : \"info\"}):\n",
        "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
        "                    urls_list.append(urls[\"href\"])\n",
        "        except:\n",
        "            print('해당 뉴스 검색 페이지의 네이버 뉴스 링크를 모으는 중 에러가 발생했습니다. : ', url)\n",
        "\n",
        "        # urls_list : 해당 페이지에 있는 \"네이버 뉴스\"의 링크 모음(list)\n",
        "        if urls_list != []:\n",
        "            for url in urls_list:\n",
        "                try: # 특정 뉴스 기사글 하나를 크롤링하는 중 에러가 발생할 수 있습니다.ㄴ\n",
        "                    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
        "                    web_news = requests.get(url, headers=headers).content\n",
        "                    source_news = BeautifulSoup(web_news, 'html.parser')\n",
        "                    # 1) 카테고리 class=\"media_end_categorize_item\"\n",
        "                    category = source_news.find('em',{'class' : 'media_end_categorize_item'}).get_text()\n",
        "\n",
        "                    title = source_news.find('h2', {'class' : 'media_end_head_headline'}).get_text()\n",
        "                    print('Processing article : {}'.format(title))\n",
        "\n",
        "                    date = source_news.find('span', {'class' : 'media_end_head_info_datestamp_time'}).get_text()\n",
        "\n",
        "                    article = source_news.find('article', {'id' : 'dic_area'}).get_text()\n",
        "                    article = article.replace(\"\\n\", \"\")\n",
        "                    article = article.replace(\"\\'\", \"\")\n",
        "                    article = article.replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
        "                    article = article.replace(\"동영상 뉴스       \", \"\")\n",
        "                    article = article.replace(\"동영상 뉴스\", \"\")\n",
        "                    article = article.strip()\n",
        "\n",
        "                    press_company = source_news.find('em', {'class':'media_end_linked_more_point'}).get_text()\n",
        "\n",
        "                    categories.append(category)\n",
        "                    titles.append(title)\n",
        "                    dates.append(date)\n",
        "                    articles.append(article)\n",
        "                    press_companies.append(press_company)\n",
        "                    article_urls.append(url)\n",
        "                    \n",
        "                    time.sleep(1)\n",
        "                except:\n",
        "                    print('\\n*** {}번부터 {}번까지의 기사글을 크롤링하는 중 문제가 발생했습니다.'.format(current_call, current_call+9))\n",
        "                    print('*** 다음 링크의 뉴스를 크롤링하는 중 에러가 발생했습니다 : {}'.format(url))\n",
        "\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        current_call += 10\n",
        "\n",
        "\n",
        "    article_df = pd.DataFrame({'date':dates,\n",
        "                               'category':categories,\n",
        "                               'press':press_companies,\n",
        "                               'title':titles,\n",
        "                               'document':articles,\n",
        "                               'link':article_urls,\n",
        "                               })\n",
        "\n",
        "    # article_df.to_csv('result_{}.csv'.format(datetime.now().strftime('%y%m%d_%H%M')), index=False, encoding='utf-8')\n",
        "\n",
        "    print('\\n크롤링이 성공적으로 완료되었습니다!')\n",
        "    print('\\n크롤링 결과를 다음 파일에 저장하였습니다 : {}'.format(datetime.now().strftime('%y%m%d_%H%M')))\n",
        "\n",
        "    return article_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWoH3YzaJbiY",
        "outputId": "c886d3bf-04ba-46ce-d8c8-2c2ba76380b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검색어를 입력해주세요. (ex. 데이터 분석) : 인공지능\n",
            "검색 시작 날짜를 입력해주세요. (형식 : 2021.01.01) : 2023.11.28\n",
            "검색 종료 날짜를 입력해주세요. (형식 : 2021.12.31) : 2023.11.30\n",
            "정렬 타입을 입력해주세요 (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : 2\n",
            "크롤링을 원하는 전체 페이지 수를 입력해주세요. (ex. 5) : 5\n",
            "\n",
            "1번째 기사글부터 크롤링을 시작합니다.\n",
            "Processing article : [알림] 뉴스1 콘텐츠 저작권 고지\n",
            "Processing article : 연말 '인사폭풍' 부는 통신3사, 거세지는 '탈통신' 움직임\n",
            "Processing article : [제59회 중앙광고대상] AI로 연결된 세상, 친근한 메시지로 다가간 듯\n",
            "Processing article : 정의선, 미국 오토모티브뉴스 ‘자동차 산업 올해의 리더’\n",
            "\n",
            "11번째 기사글부터 크롤링을 시작합니다.\n",
            "\n",
            "21번째 기사글부터 크롤링을 시작합니다.\n",
            "\n",
            "31번째 기사글부터 크롤링을 시작합니다.\n",
            "\n",
            "41번째 기사글부터 크롤링을 시작합니다.\n",
            "\n",
            "크롤링이 성공적으로 완료되었습니다!\n",
            "\n",
            "크롤링 결과를 다음 파일에 저장하였습니다 : 231130_0912\n"
          ]
        }
      ],
      "source": [
        "query = input('검색어를 입력해주세요. (ex. 데이터 분석) : ')\n",
        "start_date = input('검색 시작 날짜를 입력해주세요. (형식 : 2021.01.01) : ')\n",
        "end_date = input('검색 종료 날짜를 입력해주세요. (형식 : 2021.12.31) : ')\n",
        "sort_type = input('정렬 타입을 입력해주세요 (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : ')\n",
        "max_page = input('크롤링을 원하는 전체 페이지 수를 입력해주세요. (ex. 5) : ')\n",
        "\n",
        "if start_date > end_date:\n",
        "    print('\\n시작 날짜는 종료 날짜보다 이후로 지정하실 수 없습니다. 다시 실행해주세요!')\n",
        "elif max_page == '':\n",
        "    max_page = 5\n",
        "    print('\\n원하시는 페이지 수가 입력되지 않았습니다. 5 페이지까지만 크롤링을 진행합니다.')\n",
        "    main_crawling(query, start_date, end_date, sort_type, max_page)\n",
        "else:\n",
        "    max_page = int(max_page)\n",
        "    scraped_news = main_crawling(query, start_date, end_date, sort_type, max_page)\n",
        "    scraped_news.to_csv('result_{}.csv'.format(datetime.now().strftime('%y%m%d_%H%M')), index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OrtxQGk-amS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNrSxnWGEaqDKwDjhL/yAll",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
